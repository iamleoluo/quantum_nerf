"""
NeRF ç¶²çµ¡æ¨¡çµ„

æä¾›å¤šç¨® NeRF ç¶²çµ¡å¯¦ç¾ï¼š
- æ¨™æº– NeRF ç¶²çµ¡
- åˆ†å±¤ NeRF ç¶²çµ¡
- è¼•é‡ç´š NeRF ç¶²çµ¡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, List, Tuple, Optional
from .base import BaseModel, QuantumReadyMixin


class NeRFNetwork(BaseModel, QuantumReadyMixin):
    """
    æ¨™æº– NeRF ç¶²çµ¡
    
    å¯¦ç¾åŸå§‹ NeRF è«–æ–‡ä¸­çš„ç¶²çµ¡æ¶æ§‹
    """
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ– NeRF ç¶²çµ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - pos_encode_dim: ä½ç½®ç·¨ç¢¼ç¶­åº¦
                - dir_encode_dim: æ–¹å‘ç·¨ç¢¼ç¶­åº¦
                - hidden_dim: éš±è—å±¤ç¶­åº¦
                - num_layers: å±¤æ•¸
                - skip_connections: è·³èºé€£æ¥å±¤ç´¢å¼•
        """
        super().__init__(config)
        QuantumReadyMixin.__init__(self)
        
        # ç²å–é…ç½®
        self.pos_encode_dim = config.get('pos_encode_dim', 63)
        self.dir_encode_dim = config.get('dir_encode_dim', 27)
        self.hidden_dim = config.get('hidden_dim', 256)
        self.num_layers = config.get('num_layers', 8)
        self.skip_connections = config.get('skip_connections', [4])
        
        # ä½ç½®è™•ç†å±¤
        self.pos_layers = nn.ModuleList()
        self.pos_layers.append(nn.Linear(self.pos_encode_dim, self.hidden_dim))
        
        for i in range(1, self.num_layers):
            if i in self.skip_connections:
                self.pos_layers.append(nn.Linear(self.hidden_dim + self.pos_encode_dim, self.hidden_dim))
            else:
                self.pos_layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))
        
        # å¯†åº¦é æ¸¬é ­
        self.density_head = nn.Linear(self.hidden_dim, 1)
        
        # ç‰¹å¾µæå–å±¤
        self.feature_layer = nn.Linear(self.hidden_dim, self.hidden_dim)
        
        # é¡è‰²é æ¸¬å±¤
        self.color_layers = nn.ModuleList([
            nn.Linear(self.hidden_dim + self.dir_encode_dim, self.hidden_dim // 2)
        ])
        self.rgb_head = nn.Linear(self.hidden_dim // 2, 3)
    
    def forward(self, pos_encoded: torch.Tensor, dir_encoded: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            pos_encoded: [batch, pos_encode_dim] ç·¨ç¢¼å¾Œçš„ä½ç½®
            dir_encoded: [batch, dir_encode_dim] ç·¨ç¢¼å¾Œçš„æ–¹å‘
            
        Returns:
            rgb: [batch, 3] é æ¸¬çš„é¡è‰²
            density: [batch, 1] é æ¸¬çš„å¯†åº¦
        """
        # è™•ç†ä½ç½®
        h = pos_encoded
        for i, layer in enumerate(self.pos_layers):
            h = layer(h)
            h = F.relu(h)
            
            # è·³èºé€£æ¥
            if i in self.skip_connections:
                h = torch.cat([h, pos_encoded], dim=-1)
        
        # é æ¸¬å¯†åº¦
        density = F.relu(self.density_head(h))
        
        # æå–ç‰¹å¾µ
        features = self.feature_layer(h)
        
        # çµåˆç‰¹å¾µå’Œæ–¹å‘
        color_input = torch.cat([features, dir_encoded], dim=-1)
        
        # è™•ç†é¡è‰²
        for layer in self.color_layers:
            color_input = F.relu(layer(color_input))
        
        # é æ¸¬ RGB
        rgb = torch.sigmoid(self.rgb_head(color_input))
        
        return rgb, density
    
    def quantum_forward(self, pos_encoded: torch.Tensor, dir_encoded: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é‡å­å¢å¼·çš„å‰å‘å‚³æ’­
        
        Args:
            pos_encoded: [batch, pos_encode_dim] ç·¨ç¢¼å¾Œçš„ä½ç½®
            dir_encoded: [batch, dir_encode_dim] ç·¨ç¢¼å¾Œçš„æ–¹å‘
            
        Returns:
            rgb: [batch, 3] é æ¸¬çš„é¡è‰²
            density: [batch, 1] é æ¸¬çš„å¯†åº¦
        """
        if not self.use_quantum:
            return self.forward(pos_encoded, dir_encoded)
        
        # ä½¿ç”¨é‡å­å±¤è™•ç†ä½ç½®å’Œæ–¹å‘
        pos_encoded = super().quantum_forward(pos_encoded)
        dir_encoded = super().quantum_forward(dir_encoded)
        
        # ç¹¼çºŒæ¨™æº–å‰å‘å‚³æ’­
        return self.forward(pos_encoded, dir_encoded)


class HierarchicalNeRF(BaseModel, QuantumReadyMixin):
    """
    åˆ†å±¤ NeRF ç¶²çµ¡
    
    å¯¦ç¾ç²—ç´°å…©éšæ®µæ¡æ¨£çš„ NeRF ç¶²çµ¡
    """
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–åˆ†å±¤ NeRF ç¶²çµ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - pos_encode_dim: ä½ç½®ç·¨ç¢¼ç¶­åº¦
                - dir_encode_dim: æ–¹å‘ç·¨ç¢¼ç¶­åº¦
                - hidden_dim: éš±è—å±¤ç¶­åº¦
                - num_layers: å±¤æ•¸
                - skip_connections: è·³èºé€£æ¥å±¤ç´¢å¼•
        """
        super().__init__(config)
        QuantumReadyMixin.__init__(self)
        
        # å‰µå»ºç²—ç¶²çµ¡å’Œç´°ç¶²çµ¡
        self.coarse_net = NeRFNetwork(config)
        self.fine_net = NeRFNetwork(config)
    
    def forward(self, pos_encoded: torch.Tensor, dir_encoded: torch.Tensor,
                z_vals: torch.Tensor, rays_d: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            pos_encoded: [batch, n_samples, pos_encode_dim] ç·¨ç¢¼å¾Œçš„ä½ç½®
            dir_encoded: [batch, n_samples, dir_encode_dim] ç·¨ç¢¼å¾Œçš„æ–¹å‘
            z_vals: [batch, n_samples] æ¡æ¨£é»çš„æ·±åº¦å€¼
            rays_d: [batch, 3] å°„ç·šæ–¹å‘
            
        Returns:
            rgb_coarse: [batch, 3] ç²—ç¶²çµ¡é æ¸¬çš„é¡è‰²
            rgb_fine: [batch, 3] ç´°ç¶²çµ¡é æ¸¬çš„é¡è‰²
            weights: [batch, n_samples] é‡è¦æ€§æ¬Šé‡
        """
        # ç²—ç¶²çµ¡å‰å‘å‚³æ’­
        rgb_coarse, density_coarse = self.coarse_net(pos_encoded, dir_encoded)
        
        # è¨ˆç®—ç²—ç¶²çµ¡çš„æ¬Šé‡
        weights = self.compute_weights(density_coarse, z_vals, rays_d)
        
        # é‡è¦æ€§æ¡æ¨£
        z_vals_fine = self.importance_sampling(z_vals, weights)
        
        # ç´°ç¶²çµ¡å‰å‘å‚³æ’­
        pos_encoded_fine = self.encode_positions(z_vals_fine, rays_d)
        dir_encoded_fine = self.encode_directions(rays_d)
        rgb_fine, _ = self.fine_net(pos_encoded_fine, dir_encoded_fine)
        
        return rgb_coarse, rgb_fine, weights
    
    def compute_weights(self, density: torch.Tensor, z_vals: torch.Tensor,
                       rays_d: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—é«”ç©æ¸²æŸ“æ¬Šé‡
        
        Args:
            density: [batch, n_samples, 1] å¯†åº¦å€¼
            z_vals: [batch, n_samples] æ·±åº¦å€¼
            rays_d: [batch, 3] å°„ç·šæ–¹å‘
            
        Returns:
            weights: [batch, n_samples] æ¬Šé‡
        """
        # è¨ˆç®—ç›¸é„°æ¡æ¨£é»ä¹‹é–“çš„è·é›¢
        dists = z_vals[..., 1:] - z_vals[..., :-1]
        dists = torch.cat([dists, torch.tensor([1e10], device=dists.device).expand(dists[..., :1].shape)], -1)
        
        # è€ƒæ…®å°„ç·šæ–¹å‘
        dists = dists * torch.norm(rays_d[..., None, :], dim=-1)
        
        # è¨ˆç®— alpha å€¼
        alpha = 1. - torch.exp(-density[..., 0] * dists)
        
        # è¨ˆç®—é€å°„ç‡
        transmittance = torch.cumprod(
            torch.cat([torch.ones((alpha.shape[0], 1), device=alpha.device), 1. - alpha + 1e-10], -1), -1
        )[:, :-1]
        
        # è¨ˆç®—æ¬Šé‡
        weights = alpha * transmittance
        
        return weights
    
    def importance_sampling(self, z_vals: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """
        é‡è¦æ€§æ¡æ¨£
        
        Args:
            z_vals: [batch, n_samples] åŸå§‹æ·±åº¦å€¼
            weights: [batch, n_samples] æ¬Šé‡
            
        Returns:
            z_vals_fine: [batch, n_samples] æ–°çš„æ·±åº¦å€¼
        """
        # è¨ˆç®—ç´¯ç©åˆ†å¸ƒå‡½æ•¸
        cdf = torch.cumsum(weights, dim=-1)
        cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)
        
        # å‡å‹»æ¡æ¨£
        u = torch.linspace(0., 1., steps=z_vals.shape[-1], device=z_vals.device)
        u = u.expand(list(z_vals.shape[:-1]) + [z_vals.shape[-1]])
        
        # åè½‰ CDF å¾—åˆ°æ–°çš„æ¡æ¨£é»
        inds = torch.searchsorted(cdf, u, right=True)
        below = torch.max(torch.zeros_like(inds-1), inds-1)
        above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)
        inds_g = torch.stack([below, above], -1)
        
        # ç·šæ€§æ’å€¼
        cdf_g = torch.gather(cdf, -1, inds_g)
        z_vals_g = torch.gather(z_vals, -1, inds_g)
        
        denom = (cdf_g[..., 1] - cdf_g[..., 0])
        denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)
        t = (u - cdf_g[..., 0]) / denom
        z_vals_fine = z_vals_g[..., 0] + t * (z_vals_g[..., 1] - z_vals_g[..., 0])
        
        return z_vals_fine


class CompactNeRF(BaseModel, QuantumReadyMixin):
    """
    è¼•é‡ç´š NeRF ç¶²çµ¡
    
    å¯¦ç¾ä¸€å€‹æ›´å°æ›´å¿«çš„ NeRF ç¶²çµ¡
    """
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–è¼•é‡ç´š NeRF ç¶²çµ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - pos_encode_dim: ä½ç½®ç·¨ç¢¼ç¶­åº¦
                - dir_encode_dim: æ–¹å‘ç·¨ç¢¼ç¶­åº¦
                - hidden_dim: éš±è—å±¤ç¶­åº¦
                - num_layers: å±¤æ•¸
        """
        super().__init__(config)
        QuantumReadyMixin.__init__(self)
        
        # ç²å–é…ç½®
        self.pos_encode_dim = config.get('pos_encode_dim', 63)
        self.dir_encode_dim = config.get('dir_encode_dim', 27)
        self.hidden_dim = config.get('hidden_dim', 128)
        self.num_layers = config.get('num_layers', 4)
        
        # å…±äº«ç‰¹å¾µæå–å±¤
        self.feature_net = nn.Sequential(
            nn.Linear(self.pos_encode_dim, self.hidden_dim),
            nn.ReLU(),
            *[nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim),
                nn.ReLU()
            ) for _ in range(self.num_layers-2)],
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
        
        # å¯†åº¦é æ¸¬é ­
        self.density_head = nn.Linear(self.hidden_dim, 1)
        
        # é¡è‰²é æ¸¬é ­
        self.color_head = nn.Sequential(
            nn.Linear(self.hidden_dim + self.dir_encode_dim, self.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(self.hidden_dim // 2, 3)
        )
    
    def forward(self, pos_encoded: torch.Tensor, dir_encoded: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            pos_encoded: [batch, pos_encode_dim] ç·¨ç¢¼å¾Œçš„ä½ç½®
            dir_encoded: [batch, dir_encode_dim] ç·¨ç¢¼å¾Œçš„æ–¹å‘
            
        Returns:
            rgb: [batch, 3] é æ¸¬çš„é¡è‰²
            density: [batch, 1] é æ¸¬çš„å¯†åº¦
        """
        # æå–ç‰¹å¾µ
        features = self.feature_net(pos_encoded)
        
        # é æ¸¬å¯†åº¦
        density = F.relu(self.density_head(features))
        
        # é æ¸¬é¡è‰²
        color_input = torch.cat([features, dir_encoded], dim=-1)
        rgb = torch.sigmoid(self.color_head(color_input))
        
        return rgb, density
    
    def quantum_forward(self, pos_encoded: torch.Tensor, dir_encoded: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é‡å­å¢å¼·çš„å‰å‘å‚³æ’­
        
        Args:
            pos_encoded: [batch, pos_encode_dim] ç·¨ç¢¼å¾Œçš„ä½ç½®
            dir_encoded: [batch, dir_encode_dim] ç·¨ç¢¼å¾Œçš„æ–¹å‘
            
        Returns:
            rgb: [batch, 3] é æ¸¬çš„é¡è‰²
            density: [batch, 1] é æ¸¬çš„å¯†åº¦
        """
        if not self.use_quantum:
            return self.forward(pos_encoded, dir_encoded)
        
        # ä½¿ç”¨é‡å­å±¤è™•ç†ä½ç½®å’Œæ–¹å‘
        pos_encoded = super().quantum_forward(pos_encoded)
        dir_encoded = super().quantum_forward(dir_encoded)
        
        # ç¹¼çºŒæ¨™æº–å‰å‘å‚³æ’­
        return self.forward(pos_encoded, dir_encoded)


def create_nerf_network(config: dict) -> BaseModel:
    """
    æ ¹æ“šé…ç½®å‰µå»º NeRF ç¶²çµ¡
    
    Args:
        config: ç¶²çµ¡é…ç½®å­—å…¸
        
    Returns:
        network: NeRF ç¶²çµ¡å¯¦ä¾‹
    """
    network_type = config.get('type', 'standard')
    
    if network_type == 'standard':
        return NeRFNetwork(config)
    elif network_type == 'hierarchical':
        return HierarchicalNeRF(config)
    elif network_type == 'compact':
        return CompactNeRF(config)
    else:
        raise ValueError(f"æœªçŸ¥çš„ç¶²çµ¡é¡å‹: {network_type}")


# è¼”åŠ©å‡½æ•¸
def test_network_forward(network: BaseModel, batch_size: int = 1024):
    """
    æ¸¬è©¦ç¶²çµ¡å‰å‘å‚³æ’­
    
    Args:
        network: NeRF ç¶²çµ¡
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    print(f"ğŸ§ª æ¸¬è©¦ç¶²çµ¡å‰å‘å‚³æ’­:")
    
    # å‰µå»ºæ¸¬è©¦è¼¸å…¥
    pos_encoded = torch.randn(batch_size, network.pos_encode_dim)
    dir_encoded = torch.randn(batch_size, network.dir_encode_dim)
    
    # å‰å‘å‚³æ’­
    with torch.no_grad():
        rgb, density = network(pos_encoded, dir_encoded)
    
    print(f"   - è¼¸å…¥å½¢ç‹€: pos {pos_encoded.shape}, dir {dir_encoded.shape}")
    print(f"   - è¼¸å‡ºå½¢ç‹€: rgb {rgb.shape}, density {density.shape}")
    print(f"   - RGB ç¯„åœ: [{rgb.min():.3f}, {rgb.max():.3f}]")
    print(f"   - å¯†åº¦ç¯„åœ: [{density.min():.3f}, {density.max():.3f}]")
    
    return rgb, density


def analyze_network_gradients(network: BaseModel, pos_encoded: torch.Tensor, 
                            dir_encoded: torch.Tensor, target_rgb: torch.Tensor):
    """
    åˆ†æç¶²çµ¡æ¢¯åº¦
    
    Args:
        network: NeRF ç¶²çµ¡
        pos_encoded: ç·¨ç¢¼ä½ç½®
        dir_encoded: ç·¨ç¢¼æ–¹å‘
        target_rgb: ç›®æ¨™é¡è‰²
    """
    print(f"ğŸ“Š åˆ†æç¶²çµ¡æ¢¯åº¦:")
    
    # å‰å‘å‚³æ’­
    rgb, density = network(pos_encoded, dir_encoded)
    
    # è¨ˆç®—æå¤±
    loss = F.mse_loss(rgb, target_rgb)
    
    # åå‘å‚³æ’­
    loss.backward()
    
    # åˆ†ææ¢¯åº¦
    total_grad_norm = 0
    param_count = 0
    
    for name, param in network.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            total_grad_norm += grad_norm
            param_count += 1
            
            if 'weight' in name:
                print(f"   - {name}: æ¢¯åº¦ç¯„æ•¸ = {grad_norm:.6f}")
    
    avg_grad_norm = total_grad_norm / param_count if param_count > 0 else 0
    print(f"   - å¹³å‡æ¢¯åº¦ç¯„æ•¸: {avg_grad_norm:.6f}")
    print(f"   - æå¤±å€¼: {loss.item():.6f}")
    
    return loss.item(), avg_grad_norm 