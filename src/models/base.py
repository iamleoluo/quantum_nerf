"""
åŸºç¤æ¨¡å‹é¡

å®šç¾©æ‰€æœ‰æ¨¡å‹çš„å…±åŒæ¥å£å’ŒåŸºç¤åŠŸèƒ½
"""

import torch
import torch.nn as nn
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional


class BaseModel(nn.Module, ABC):
    """
    æ‰€æœ‰æ¨¡å‹çš„åŸºç¤é¡
    
    æä¾›å…±åŒçš„æ¥å£å’ŒåŠŸèƒ½ï¼š
    - æ¨¡å‹ä¿å­˜å’ŒåŠ è¼‰
    - åƒæ•¸çµ±è¨ˆ
    - è¨­å‚™ç®¡ç†
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    @abstractmethod
    def forward(self, *args, **kwargs):
        """å‰å‘å‚³æ’­ - å­é¡å¿…é ˆå¯¦ç¾"""
        pass
    
    def count_parameters(self) -> int:
        """è¨ˆç®—æ¨¡å‹åƒæ•¸æ•¸é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def save_model(self, path: str, epoch: Optional[int] = None, 
                   optimizer_state: Optional[Dict] = None):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            path: ä¿å­˜è·¯å¾‘
            epoch: ç•¶å‰è¨“ç·´è¼ªæ•¸
            optimizer_state: å„ªåŒ–å™¨ç‹€æ…‹
        """
        save_dict = {
            'model_state_dict': self.state_dict(),
            'config': self.config,
            'model_class': self.__class__.__name__
        }
        
        if epoch is not None:
            save_dict['epoch'] = epoch
            
        if optimizer_state is not None:
            save_dict['optimizer_state_dict'] = optimizer_state
            
        torch.save(save_dict, path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {path}")
    
    def load_model(self, path: str, strict: bool = True) -> Dict[str, Any]:
        """
        åŠ è¼‰æ¨¡å‹
        
        Args:
            path: æ¨¡å‹è·¯å¾‘
            strict: æ˜¯å¦åš´æ ¼åŒ¹é…åƒæ•¸
            
        Returns:
            åŠ è¼‰çš„é¡å¤–ä¿¡æ¯ï¼ˆå¦‚ epoch, optimizer_state ç­‰ï¼‰
        """
        checkpoint = torch.load(path, map_location=self.device)
        self.load_state_dict(checkpoint['model_state_dict'], strict=strict)
        
        print(f"âœ… æ¨¡å‹å·²å¾ {path} åŠ è¼‰")
        
        # è¿”å›é¡å¤–ä¿¡æ¯
        extra_info = {}
        for key in ['epoch', 'optimizer_state_dict', 'config']:
            if key in checkpoint:
                extra_info[key] = checkpoint[key]
                
        return extra_info
    
    def to_device(self, device: Optional[torch.device] = None):
        """ç§»å‹•æ¨¡å‹åˆ°æŒ‡å®šè¨­å‚™"""
        if device is None:
            device = self.device
        self.to(device)
        self.device = device
        return self
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_class': self.__class__.__name__,
            'parameters': self.count_parameters(),
            'device': str(self.device),
            'config': self.config
        }
    
    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        info = self.get_model_info()
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   é¡åˆ¥: {info['model_class']}")
        print(f"   åƒæ•¸æ•¸é‡: {info['parameters']:,}")
        print(f"   è¨­å‚™: {info['device']}")
        print(f"   é…ç½®: {info['config']}")


class QuantumReadyMixin:
    """
    é‡å­å°±ç·’æ··åˆé¡
    
    ç‚ºæ¨¡å‹æ·»åŠ é‡å­è¨ˆç®—æ•´åˆçš„æ¥å£
    """
    
    def __init__(self):
        self.quantum_enabled = False
        self.quantum_components = {}
    
    def enable_quantum(self, components: Dict[str, Any]):
        """å•Ÿç”¨é‡å­çµ„ä»¶"""
        self.quantum_enabled = True
        self.quantum_components = components
        print("ğŸŒŒ é‡å­çµ„ä»¶å·²å•Ÿç”¨")
    
    def disable_quantum(self):
        """ç¦ç”¨é‡å­çµ„ä»¶"""
        self.quantum_enabled = False
        self.quantum_components = {}
        print("ğŸ’» åˆ‡æ›å›ç¶“å…¸è¨ˆç®—")
    
    def is_quantum_enabled(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦å•Ÿç”¨é‡å­è¨ˆç®—"""
        return self.quantum_enabled 