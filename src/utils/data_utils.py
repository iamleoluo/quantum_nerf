"""
æ•¸æ“šè™•ç†å·¥å…·æ¨¡çµ„

è™•ç† NeRF è¨“ç·´æ‰€éœ€çš„å„ç¨®æ•¸æ“šï¼š
- åœ–åƒåŠ è¼‰å’Œé è™•ç†
- ç›¸æ©Ÿåƒæ•¸è§£æ
- å°„ç·šç”Ÿæˆ
- æ•¸æ“šé›†åˆ†å‰²
"""

import torch
import numpy as np
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import imageio
from PIL import Image


class NeRFDataLoader:
    """
    NeRF æ•¸æ“šåŠ è¼‰å™¨
    
    è² è²¬åŠ è¼‰å’Œé è™•ç† NeRF è¨“ç·´æ‰€éœ€çš„æ‰€æœ‰æ•¸æ“š
    """
    
    def __init__(self, data_dir: str, scene_name: str = "lego", 
                 image_scale: float = 1.0, white_background: bool = True):
        """
        åˆå§‹åŒ–æ•¸æ“šåŠ è¼‰å™¨
        
        Args:
            data_dir: æ•¸æ“šç›®éŒ„è·¯å¾‘
            scene_name: å ´æ™¯åç¨±
            image_scale: åœ–åƒç¸®æ”¾æ¯”ä¾‹
            white_background: æ˜¯å¦ä½¿ç”¨ç™½è‰²èƒŒæ™¯
        """
        self.data_dir = Path(data_dir)
        self.scene_name = scene_name
        self.image_scale = image_scale
        self.white_background = white_background
        
        # æ•¸æ“šå­˜å„²
        self.images = {}
        self.poses = {}
        self.intrinsics = {}
        self.image_size = None
        
        print(f"ğŸ“‚ åˆå§‹åŒ–æ•¸æ“šåŠ è¼‰å™¨: {scene_name}")
    
    def load_images(self, split: str = "train") -> torch.Tensor:
        """
        åŠ è¼‰åœ–åƒæ•¸æ“š
        
        Args:
            split: æ•¸æ“šåˆ†å‰² ("train", "val", "test")
            
        Returns:
            images: [N, H, W, 3] åœ–åƒå¼µé‡
        """
        print(f"ğŸ“¸ åŠ è¼‰ {split} åœ–åƒ...")
        
        image_dir = self.data_dir / split
        if not image_dir.exists():
            raise FileNotFoundError(f"åœ–åƒç›®éŒ„ä¸å­˜åœ¨: {image_dir}")
        
        # ç²å–æ‰€æœ‰åœ–åƒæ–‡ä»¶
        image_files = sorted(list(image_dir.glob("*.jpg")) + 
                           list(image_dir.glob("*.png")))
        
        if len(image_files) == 0:
            raise ValueError(f"åœ¨ {image_dir} ä¸­æœªæ‰¾åˆ°åœ–åƒæ–‡ä»¶")
        
        images = []
        for img_file in image_files:
            # åŠ è¼‰åœ–åƒ
            img = imageio.imread(img_file)
            
            # è½‰æ›ç‚º RGB (å¦‚æœæ˜¯ RGBA)
            if img.shape[-1] == 4:
                if self.white_background:
                    # ç™½è‰²èƒŒæ™¯æ··åˆ
                    img = img[..., :3] * img[..., -1:] / 255.0 + (1.0 - img[..., -1:] / 255.0)
                    img = (img * 255).astype(np.uint8)
                else:
                    img = img[..., :3]
            
            # ç¸®æ”¾åœ–åƒ
            if self.image_scale != 1.0:
                h, w = img.shape[:2]
                new_h, new_w = int(h * self.image_scale), int(w * self.image_scale)
                img = np.array(Image.fromarray(img).resize((new_w, new_h)))
            
            # æ¨™æº–åŒ–åˆ° [0, 1]
            img = img.astype(np.float32) / 255.0
            images.append(img)
        
        images = np.stack(images, axis=0)
        self.images[split] = torch.from_numpy(images)
        self.image_size = images.shape[1:3]  # (H, W)
        
        print(f"âœ… åŠ è¼‰äº† {len(images)} å¼µ {split} åœ–åƒï¼Œå°ºå¯¸: {self.image_size}")
        return self.images[split]
    
    def load_camera_parameters(self, split: str = "train") -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åŠ è¼‰ç›¸æ©Ÿåƒæ•¸
        
        Args:
            split: æ•¸æ“šåˆ†å‰²
            
        Returns:
            poses: [N, 4, 4] ç›¸æ©Ÿä½å§¿çŸ©é™£
            intrinsics: [N, 3, 3] ç›¸æ©Ÿå…§åƒçŸ©é™£
        """
        print(f"ğŸ“· åŠ è¼‰ {split} ç›¸æ©Ÿåƒæ•¸...")
        
        # åŠ è¼‰ä½å§¿æ•¸æ“š
        poses_file = self.data_dir / f"transforms_{split}.json"
        if not poses_file.exists():
            raise FileNotFoundError(f"ç›¸æ©Ÿåƒæ•¸æ–‡ä»¶ä¸å­˜åœ¨: {poses_file}")
        
        with open(poses_file, 'r') as f:
            meta = json.load(f)
        
        poses = []
        intrinsics = []
        
        # æå–ç›¸æ©Ÿåƒæ•¸
        camera_angle_x = meta.get('camera_angle_x', 0.6911112070083618)
        
        for frame in meta['frames']:
            # ä½å§¿çŸ©é™£
            pose = np.array(frame['transform_matrix'], dtype=np.float32)
            poses.append(pose)
            
            # å…§åƒçŸ©é™£
            if self.image_size is not None:
                h, w = self.image_size
                focal = 0.5 * w / np.tan(0.5 * camera_angle_x)
                
                intrinsic = np.array([
                    [focal, 0, w/2],
                    [0, focal, h/2],
                    [0, 0, 1]
                ], dtype=np.float32)
                intrinsics.append(intrinsic)
        
        poses = np.stack(poses, axis=0)
        intrinsics = np.stack(intrinsics, axis=0) if intrinsics else None
        
        self.poses[split] = torch.from_numpy(poses)
        if intrinsics is not None:
            self.intrinsics[split] = torch.from_numpy(intrinsics)
        
        print(f"âœ… åŠ è¼‰äº† {len(poses)} å€‹ç›¸æ©Ÿä½å§¿")
        return self.poses[split], self.intrinsics.get(split)
    
    def create_rays(self, poses: torch.Tensor, intrinsics: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ç‚ºçµ¦å®šçš„ç›¸æ©Ÿä½å§¿å‰µå»ºå°„ç·š
        
        Args:
            poses: [N, 4, 4] ç›¸æ©Ÿä½å§¿
            intrinsics: [N, 3, 3] ç›¸æ©Ÿå…§åƒ (å¯é¸)
            
        Returns:
            rays_o: [N, H, W, 3] å°„ç·šèµ·é»
            rays_d: [N, H, W, 3] å°„ç·šæ–¹å‘
        """
        if self.image_size is None:
            raise ValueError("è«‹å…ˆåŠ è¼‰åœ–åƒä»¥ç¢ºå®šåœ–åƒå°ºå¯¸")
        
        h, w = self.image_size
        n_poses = poses.shape[0]
        
        # å‰µå»ºåƒç´ åº§æ¨™ç¶²æ ¼
        i, j = torch.meshgrid(
            torch.linspace(0, w-1, w),
            torch.linspace(0, h-1, h),
            indexing='xy'
        )
        
        rays_o_list = []
        rays_d_list = []
        
        for idx in range(n_poses):
            pose = poses[idx]
            
            if intrinsics is not None:
                # ä½¿ç”¨æä¾›çš„å…§åƒ
                intrinsic = intrinsics[idx]
                focal_x, focal_y = intrinsic[0, 0], intrinsic[1, 1]
                cx, cy = intrinsic[0, 2], intrinsic[1, 2]
            else:
                # ä½¿ç”¨é»˜èªå…§åƒ
                focal_x = focal_y = 0.5 * w / np.tan(0.5 * 0.6911112070083618)
                cx, cy = w/2, h/2
            
            # è½‰æ›ç‚ºç›¸æ©Ÿåº§æ¨™ç³»
            dirs = torch.stack([
                (i - cx) / focal_x,
                -(j - cy) / focal_y,
                -torch.ones_like(i)
            ], dim=-1)
            
            # è½‰æ›åˆ°ä¸–ç•Œåº§æ¨™ç³»
            rays_d = torch.sum(dirs[..., None, :] * pose[:3, :3], dim=-1)
            rays_o = pose[:3, -1].expand(rays_d.shape)
            
            rays_o_list.append(rays_o)
            rays_d_list.append(rays_d)
        
        rays_o = torch.stack(rays_o_list, dim=0)
        rays_d = torch.stack(rays_d_list, dim=0)
        
        return rays_o, rays_d
    
    def get_training_data(self, batch_size: int = 1024) -> Dict[str, torch.Tensor]:
        """
        ç²å–è¨“ç·´æ•¸æ“šæ‰¹æ¬¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            batch_data: åŒ…å«å°„ç·šå’Œç›®æ¨™é¡è‰²çš„å­—å…¸
        """
        if "train" not in self.images or "train" not in self.poses:
            raise ValueError("è«‹å…ˆåŠ è¼‰è¨“ç·´æ•¸æ“š")
        
        images = self.images["train"]
        poses = self.poses["train"]
        intrinsics = self.intrinsics.get("train")
        
        # å‰µå»ºå°„ç·š
        rays_o, rays_d = self.create_rays(poses, intrinsics)
        
        # éš¨æ©Ÿé¸æ“‡åœ–åƒ
        n_images = images.shape[0]
        img_idx = torch.randint(0, n_images, (1,)).item()
        
        # ç²å–è©²åœ–åƒçš„æ•¸æ“š
        target_img = images[img_idx]  # [H, W, 3]
        img_rays_o = rays_o[img_idx]  # [H, W, 3]
        img_rays_d = rays_d[img_idx]  # [H, W, 3]
        
        # å¹³å¦åŒ–
        h, w = target_img.shape[:2]
        target_rgb = target_img.reshape(-1, 3)  # [H*W, 3]
        rays_o_flat = img_rays_o.reshape(-1, 3)  # [H*W, 3]
        rays_d_flat = img_rays_d.reshape(-1, 3)  # [H*W, 3]
        
        # éš¨æ©Ÿæ¡æ¨£å°„ç·š
        total_rays = h * w
        ray_indices = torch.randperm(total_rays)[:batch_size]
        
        batch_data = {
            'rays_o': rays_o_flat[ray_indices],      # [batch_size, 3]
            'rays_d': rays_d_flat[ray_indices],      # [batch_size, 3]
            'target_rgb': target_rgb[ray_indices],   # [batch_size, 3]
            'img_idx': img_idx,
            'ray_indices': ray_indices
        }
        
        return batch_data
    
    def get_validation_data(self, img_idx: int = 0) -> Dict[str, torch.Tensor]:
        """
        ç²å–é©—è­‰æ•¸æ“š
        
        Args:
            img_idx: åœ–åƒç´¢å¼•
            
        Returns:
            val_data: é©—è­‰æ•¸æ“šå­—å…¸
        """
        if "val" not in self.images or "val" not in self.poses:
            raise ValueError("è«‹å…ˆåŠ è¼‰é©—è­‰æ•¸æ“š")
        
        images = self.images["val"]
        poses = self.poses["val"]
        intrinsics = self.intrinsics.get("val")
        
        # å‰µå»ºå°„ç·š
        rays_o, rays_d = self.create_rays(poses, intrinsics)
        
        # ç²å–æŒ‡å®šåœ–åƒçš„æ•¸æ“š
        target_img = images[img_idx]
        img_rays_o = rays_o[img_idx]
        img_rays_d = rays_d[img_idx]
        
        val_data = {
            'rays_o': img_rays_o,        # [H, W, 3]
            'rays_d': img_rays_d,        # [H, W, 3]
            'target_rgb': target_img,    # [H, W, 3]
            'img_idx': img_idx
        }
        
        return val_data


def create_synthetic_dataset(n_images: int = 100, image_size: Tuple[int, int] = (64, 64),
                           output_dir: str = "data/synthetic") -> Dict[str, np.ndarray]:
    """
    å‰µå»ºåˆæˆæ•¸æ“šé›†ç”¨æ–¼æ¼”ç¤º
    
    Args:
        n_images: åœ–åƒæ•¸é‡
        image_size: åœ–åƒå°ºå¯¸ (H, W)
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        dataset: åŒ…å«åœ–åƒå’Œç›¸æ©Ÿåƒæ•¸çš„å­—å…¸
    """
    print(f"ğŸ¨ å‰µå»ºåˆæˆæ•¸æ“šé›†: {n_images} å¼µåœ–åƒï¼Œå°ºå¯¸ {image_size}")
    
    h, w = image_size
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å‰µå»ºç›¸æ©Ÿè»Œè·¡ (åœ“å½¢)
    angles = np.linspace(0, 2*np.pi, n_images, endpoint=False)
    radius = 4.0
    height = 1.0
    
    images = []
    poses = []
    
    for i, angle in enumerate(angles):
        # ç›¸æ©Ÿä½ç½®
        cam_pos = np.array([
            radius * np.cos(angle),
            height,
            radius * np.sin(angle)
        ])
        
        # æœå‘åŸé»
        look_at = np.array([0., 0., 0.])
        up = np.array([0., 1., 0.])
        
        # å‰µå»ºç›¸æ©Ÿåˆ°ä¸–ç•Œçš„è®Šæ›çŸ©é™£
        z_axis = cam_pos - look_at
        z_axis = z_axis / np.linalg.norm(z_axis)
        x_axis = np.cross(up, z_axis)
        x_axis = x_axis / np.linalg.norm(x_axis)
        y_axis = np.cross(z_axis, x_axis)
        
        pose = np.eye(4)
        pose[:3, 0] = x_axis
        pose[:3, 1] = y_axis
        pose[:3, 2] = z_axis
        pose[:3, 3] = cam_pos
        
        poses.append(pose)
        
        # å‰µå»ºç°¡å–®çš„åˆæˆåœ–åƒ (å½©è‰²æ–¹å¡Š)
        img = np.zeros((h, w, 3))
        
        # æ·»åŠ ä¸€äº›å½©è‰²å€åŸŸ
        quarter_h, quarter_w = h//4, w//4
        
        # ç´…è‰²å€åŸŸ
        img[quarter_h:quarter_h*2, quarter_w:quarter_w*2] = [1.0, 0.0, 0.0]
        # ç¶ è‰²å€åŸŸ  
        img[quarter_h:quarter_h*2, quarter_w*2:quarter_w*3] = [0.0, 1.0, 0.0]
        # è—è‰²å€åŸŸ
        img[quarter_h*2:quarter_h*3, quarter_w:quarter_w*2] = [0.0, 0.0, 1.0]
        # é»ƒè‰²å€åŸŸ
        img[quarter_h*2:quarter_h*3, quarter_w*2:quarter_w*3] = [1.0, 1.0, 0.0]
        
        # æ·»åŠ ä¸€äº›éš¨æ©Ÿå™ªè²
        noise = np.random.normal(0, 0.05, img.shape)
        img = np.clip(img + noise, 0, 1)
        
        images.append(img)
    
    images = np.stack(images)
    poses = np.stack(poses)
    
    # ä¿å­˜æ•¸æ“š
    dataset = {
        'images': images,
        'poses': poses,
        'image_size': image_size,
        'n_images': n_images
    }
    
    # ä¿å­˜ç‚º numpy æ ¼å¼
    np.savez(output_path / 'synthetic_dataset.npz', **dataset)
    
    # å‰µå»º transforms.json æ ¼å¼çš„ç›¸æ©Ÿåƒæ•¸
    camera_angle_x = 0.6911112070083618
    
    transforms = {
        'camera_angle_x': camera_angle_x,
        'frames': []
    }
    
    for i, pose in enumerate(poses):
        frame = {
            'file_path': f'./images/img_{i:03d}',
            'transform_matrix': pose.tolist()
        }
        transforms['frames'].append(frame)
    
    # åˆ†å‰²æ•¸æ“šé›†
    n_train = int(0.8 * n_images)
    n_val = int(0.1 * n_images)
    
    # è¨“ç·´é›†
    train_transforms = {
        'camera_angle_x': camera_angle_x,
        'frames': transforms['frames'][:n_train]
    }
    
    # é©—è­‰é›†
    val_transforms = {
        'camera_angle_x': camera_angle_x,
        'frames': transforms['frames'][n_train:n_train+n_val]
    }
    
    # æ¸¬è©¦é›†
    test_transforms = {
        'camera_angle_x': camera_angle_x,
        'frames': transforms['frames'][n_train+n_val:]
    }
    
    # ä¿å­˜ JSON æ–‡ä»¶
    with open(output_path / 'transforms_train.json', 'w') as f:
        json.dump(train_transforms, f, indent=2)
    
    with open(output_path / 'transforms_val.json', 'w') as f:
        json.dump(val_transforms, f, indent=2)
        
    with open(output_path / 'transforms_test.json', 'w') as f:
        json.dump(test_transforms, f, indent=2)
    
    # ä¿å­˜åœ–åƒæ–‡ä»¶
    for split, split_transforms in [('train', train_transforms), 
                                   ('val', val_transforms), 
                                   ('test', test_transforms)]:
        split_dir = output_path / split
        split_dir.mkdir(exist_ok=True)
        
        for i, frame in enumerate(split_transforms['frames']):
            img_idx = int(frame['file_path'].split('_')[-1])
            img = (images[img_idx] * 255).astype(np.uint8)
            imageio.imwrite(split_dir / f'img_{i:03d}.png', img)
    
    print(f"âœ… åˆæˆæ•¸æ“šé›†å·²ä¿å­˜åˆ°: {output_path}")
    print(f"   - è¨“ç·´é›†: {len(train_transforms['frames'])} å¼µåœ–åƒ")
    print(f"   - é©—è­‰é›†: {len(val_transforms['frames'])} å¼µåœ–åƒ") 
    print(f"   - æ¸¬è©¦é›†: {len(test_transforms['frames'])} å¼µåœ–åƒ")
    
    return dataset


# è¼”åŠ©å‡½æ•¸
def create_rays(height: int, width: int, focal: float, pose: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ç‚ºå–®å€‹ç›¸æ©Ÿä½å§¿å‰µå»ºå°„ç·š
    
    Args:
        height, width: åœ–åƒå°ºå¯¸
        focal: ç„¦è·
        pose: [4, 4] ç›¸æ©Ÿä½å§¿çŸ©é™£
        
    Returns:
        rays_o: [H*W, 3] å°„ç·šèµ·é»
        rays_d: [H*W, 3] å°„ç·šæ–¹å‘
    """
    # å‰µå»ºåƒç´ åº§æ¨™
    i, j = torch.meshgrid(
        torch.linspace(0, width-1, width),
        torch.linspace(0, height-1, height),
        indexing='xy'
    )
    
    # è½‰æ›ç‚ºç›¸æ©Ÿåº§æ¨™
    dirs = torch.stack([
        (i - width * 0.5) / focal,
        -(j - height * 0.5) / focal,
        -torch.ones_like(i)
    ], -1)
    
    # è½‰æ›åˆ°ä¸–ç•Œåº§æ¨™ç³»
    rays_d = torch.sum(dirs[..., None, :] * pose[:3, :3], -1)
    rays_o = pose[:3, -1].expand(rays_d.shape)
    
    return rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)


def normalize_rays(rays_d: torch.Tensor) -> torch.Tensor:
    """
    æ¨™æº–åŒ–å°„ç·šæ–¹å‘å‘é‡
    
    Args:
        rays_d: [..., 3] å°„ç·šæ–¹å‘
        
    Returns:
        normalized_rays_d: [..., 3] æ¨™æº–åŒ–å¾Œçš„å°„ç·šæ–¹å‘
    """
    return rays_d / torch.norm(rays_d, dim=-1, keepdim=True)


def sample_rays_batch(rays_o: torch.Tensor, rays_d: torch.Tensor, 
                     target_rgb: torch.Tensor, batch_size: int) -> Dict[str, torch.Tensor]:
    """
    å¾å°„ç·šä¸­æ¡æ¨£æ‰¹æ¬¡æ•¸æ“š
    
    Args:
        rays_o: [N, 3] å°„ç·šèµ·é»
        rays_d: [N, 3] å°„ç·šæ–¹å‘  
        target_rgb: [N, 3] ç›®æ¨™é¡è‰²
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        batch_data: æ‰¹æ¬¡æ•¸æ“šå­—å…¸
    """
    n_rays = rays_o.shape[0]
    indices = torch.randperm(n_rays)[:batch_size]
    
    return {
        'rays_o': rays_o[indices],
        'rays_d': rays_d[indices],
        'target_rgb': target_rgb[indices],
        'indices': indices
    } 